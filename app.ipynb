{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loading with Langchhain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "#loading the data from a pdf file\n",
    "data=\"D:\\MIT_ADT\\Intern\\Build fast\\RAG_dataset.pdf\"\n",
    "loader=PyPDFLoader(data)\n",
    "\n",
    "#processing the data into the document that langchain can use in further tasks\n",
    "documents=loader.load()\n",
    "#print(f\"Loaded {len(documents)} documents from the datasets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 documents from the datasets.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loaded {len(documents)} documents from the datasets.\")  #give the number of pages in the pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the RAG with langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass  #propmts the user for imput and hides the text they type(API KEY)\n",
    "import os\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# ensuring that the openai key is set\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karpe\\AppData\\Local\\Temp\\ipykernel_1500\\2103928298.py:5: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings=OpenAIEmbeddings()\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings #converting the doc into vector embeddings (numerical representation)\n",
    "from langchain.vectorstores import FAISS #storing the ebedded doc into teh FAISS vector\n",
    "# Retrieve the OpenAI API key from the environment variable\n",
    "#creating the embeddings and setting up the FAISS vectorstore\n",
    "embeddings=OpenAIEmbeddings()\n",
    "vectorstore=FAISS.from_documents(documents,embeddings)\n",
    "retriever=vectorstore.as_retriever()  #Converting the faiss vector store into a retriever that can fetch relevant docs based on teh query\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chatbot construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rag pipeline setup completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karpe\\AppData\\Local\\Temp\\ipykernel_1500\\3214369249.py:11: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  chat_model = ChatOpenAI( openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"You are a helpful assistant. Based on the context below, answer the question concisely:\\n\\nContext: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\",\n",
    "    input_variables=[\"context\", \"question\"],\n",
    ")\n",
    "# Initialize the chat model (gpt-3.5-turbo or gpt-4)\n",
    "import os\n",
    "chat_model = ChatOpenAI( openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "# loading the lm \n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\") \n",
    "# Set up the retrieval QA pipeline\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",  # You could also experiment with other chain types like \"map_reduce\"\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,  # This will return the source documents alongside the answer\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    " #this retrieverQA ties LLM with a retriever(FAISS)tos et up the pipleine for answering questions\n",
    "print(\"rag pipeline setup completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karpe\\AppData\\Local\\Temp\\ipykernel_1500\\2399155932.py:3: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa({\"query\": queryy}) # retrieving the the answer  and documents by running the query\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The outline of this paper includes an introduction, Meta Chain-Of-Thought, and Towards Deliberate Reasoning With Language Models - Search.\n",
      "Source Documents: [Document(id='d724a927-c8de-433f-bafd-57a0bf8a4ba0', metadata={'source': 'D:\\\\MIT_ADT\\\\Intern\\\\Build fast\\\\RAG_dataset.pdf', 'page': 51}, page_content='Towards System 2 Reasoning in LLMs: Learning How to Think With Meta Chain-of-Thought\\n10. Acknowledgments\\nWe would like to thank Aviral Kumar, Benjamin Eysenbach, Nathan Lambert, Rishabh Agarwal, Sasha\\nRush and Noah Goodman for the fruitful discussions and feedback on this report.\\n52'), Document(id='f2b64216-181f-4b4e-a605-38840927ef17', metadata={'source': 'D:\\\\MIT_ADT\\\\Intern\\\\Build fast\\\\RAG_dataset.pdf', 'page': 2}, page_content='Towards System 2 Reasoning in LLMs: Learning How to Think With Meta Chain-of-Thought\\n8 Going Forward 43\\n8.1 The \"Big MATH\" Project . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\\n8.1.1 Data Sourcing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\\n8.1.2 Data Filtering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\\n8.2 Infrastructure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\\n8.3 Open Research Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\\n8.3.1 Open-Ended Verification And CoT Faithfulness . . . . . . . . . . . . . . . . . . 47\\n8.3.2 Process Guidance And The Verifier Gap . . . . . . . . . . . . . . . . . . . . . . 48\\n8.3.3 Scaling Laws For Reasoning And Search . . . . . . . . . . . . . . . . . . . . . 49\\n8.3.4 Meta-Search/Search 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\\n8.3.5 Reasoning with External Tools . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\\n9 Conclusion 51\\n10 Acknowledgments 52\\nA Prompting 62\\nB Regret Analysis 63\\nC Different Instruction Tuning Objectives 63\\nD MCTS Details 63\\nE Chains-Of-Thought 64\\n3'), Document(id='d6b6b435-0c06-44fa-bf33-c5af95464d06', metadata={'source': 'D:\\\\MIT_ADT\\\\Intern\\\\Build fast\\\\RAG_dataset.pdf', 'page': 16}, page_content='Towards System 2 Reasoning in LLMs: Learning How to Think With Meta Chain-of-Thought\\nFigure 7: A*planning algorithm outlinefor a simple maze navigation task, along with a state and\\naction tokenization scheme. The search representation explicitly models nodes and queue state, the\\nsearch procedure and the cost and heuristic evaluation. Source: Figure 1 in (Lehnert et al., 2024).\\nFigure 8: Model performance vs. training computewhen using the A*planning algorithm (Search\\nAugmented) vs. no search (Solution Only). We see that the search augmented models perform much\\nbetter across all training scales (charts a and b). In particular performance is consistent with the\\nsearch formulation of Section 3.4. Figure c) shows performance in terms of task complexity as maze\\nsize increases. Results are consistent with the Meta-CoT complexity argument presented in Section 2\\nand results on the HARP benchmark in Figure 1. Source: Figure 2 in (Lehnert et al., 2024).\\n17'), Document(id='43b915f2-b5e5-4001-81ec-2e6e5a013788', metadata={'source': 'D:\\\\MIT_ADT\\\\Intern\\\\Build fast\\\\RAG_dataset.pdf', 'page': 0}, page_content='Towards System 2 Reasoning in LLMs:\\nLearning How to Think With Meta\\nChain-of-Thought\\nViolet Xiang2, Charlie Snell3, Kanishk Gandhi2, Alon Albalak1, Anikait Singh2, Chase Blagden1, Duy Phung1,\\nRafael Rafailov2,1, Nathan Lile1, Dakota Mahan1, Louis Castricato1, Jan-Philipp Fr√§nken2, Nick Haber2 and\\nChelsea Finn2\\n1SynthLabs.ai, 2Stanford University,3UC Berkeley\\nWe propose a novel framework, Meta Chain-of-Thought (Meta-CoT), which extends traditional Chain-of-\\nThought (CoT) by explicitly modeling the underlying reasoning required to arrive at a particular CoT. We\\npresent empirical evidence from state-of-the-art models exhibiting behaviors consistent with in-context search,\\nand explore methods for producing Meta-CoT via process supervision, synthetic data generation, and search\\nalgorithms. Finally, we outline a concrete pipeline for training a model to produce Meta-CoTs, incorporating\\ninstruction tuning with linearized search traces and reinforcement learning post-training. Finally, we discuss\\nopen research questions, including scaling laws, verifier roles, and the potential for discovering novel reasoning\\nalgorithms. This work provides a theoretical and practical roadmap to enable Meta-CoT in LLMs, paving the\\nway for more powerful and human-like reasoning in artificial intelligence.\\nGive a man a fish and you feed him for a day;\\nteach a man to fish and you feed him for a lifetime.\\n-Proverb\\nContents\\n1 Introduction 4\\n1.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\\n1.2 Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\n2 Meta Chain-Of-Thought 6\\n2.1 Deriving The Meta-CoT Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\n2.2 Why Does (Classical) CoT Fail? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n3 Towards Deliberate Reasoning With Language Models - Search 9\\n3.1 Inference-Time Compute: Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\\n3.2 Inference-Time Compute: Verification . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\\nAuthors are ordered randomly. Correspondence to team@synthlabs.ai.\\narXiv:2501.04682v1  [cs.AI]  8 Jan 2025')]\n"
     ]
    }
   ],
   "source": [
    "queryy = \"What is the outline of this paper?\"\n",
    "\n",
    "result = qa({\"query\": queryy}) # retrieving the the answer  and documents by running the query\n",
    "answer = result['result'] #accessing the ans and source docs sepaartely\n",
    "source_documents = result['source_documents']\n",
    "\n",
    "# Display the answer and source documents\n",
    "print(\"Answer:\", answer)\n",
    "print(\"Source Documents:\", source_documents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
